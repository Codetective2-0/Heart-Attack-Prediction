# -*- coding: utf-8 -*-
"""ML_Hackathon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Codetective2-0/Heart-Attack-Prediction/blob/main/ML_Hackathon.ipynb

# **HEALTH CARE: CHECK HEART ATTACK POSSIBILITY**

Importing our dependencies
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt #data visualization
import seaborn as sns
from operator import add

from google.colab import files
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

"""Uploading our dataset. (the .csv file)"""

files.upload()

heart1 = pd.read_csv("heart.csv")

"""Handling the Missing Attributes"""

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median") #fill the median at empty spaces 
imputer.fit(heart1)
X = imputer.transform(heart1)
heart=pd.DataFrame(X, columns=heart1.columns)

heart.head(303) #view the head

"""# **Data Analysis**"""

# plot histogram to see the distribution of the data
fig = plt.figure(figsize = (15,20))
ax = fig.gca()
heart1.hist(ax = ax)
plt.show()

"""## **Feature Selection**"""

X = heart.iloc[:,:-1]
y = heart.iloc[:,-1]

bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,y)

dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)

#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns

featureScores

print(featureScores.nlargest(6,'Score'))

"""# **Feature Scaling**"""

dataset = pd.get_dummies(heart, columns = ['cp',  'ca']) #categorical data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()
columns_to_scale = ['chol',  'thalach', 'oldpeak', 'exang']
dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])
dataset = dataset.drop(['age','sex','fbs','restecg','thal','slope','trestbps'],axis=1)

dataset.head()

"""# **Splitting the Dataset**"""

from sklearn.model_selection import train_test_split
train_set, test_set  = train_test_split(dataset, test_size=0.3, random_state=42)
print(f"Rows in train set: {len(train_set)}\nRows in test set: {len(test_set)}\n")

X_train = train_set.iloc[:,0:13]  #independent columns for training 
y_train = train_set.iloc[:,-1]    #o/p for train set
X_test = test_set.iloc[:,0:13]    #independent columns for testing 
y_test = test_set.iloc[:,-1]       #o/p for test set

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import recall_score,precision_score,classification_report,roc_auc_score,roc_curve

"""# **K-Nearest Neighbours (K-NN) Model**"""

from sklearn.neighbors import KNeighborsClassifier
error_rate = []
for i in range(1,50): 
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
  

min = (np.argmin(error_rate) +1)
print(min)
plt.figure(figsize=(10,6))
plt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

knn = KNeighborsClassifier(n_neighbors=(min))
#train the model
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
print('WITH MIN K = ', min)
print('\n')
#prediction 
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))
knn_accuracy = accuracy_score(y_test,pred)

"""## **Support Vector Machine (SVM) Model**"""

from sklearn import svm
from sklearn.svm import SVC
clf = svm.SVC(kernel='linear')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

Cs = [0.001, 0.01, 0.1, 1, 10] #Hyperparameter1
gammas = [0.001, 0.01, 0.1, 10] #Hyperparameter2
param_grid = {'C': Cs, 'gamma' : gammas}
svm_clf = GridSearchCV(SVC(kernel='rbf', probability=True), param_grid, cv=10)
# train the model
svm_clf.fit(X_train,y_train)
svm_clf.best_params_

#prediction
svm_predict = svm_clf.predict(X_test)
#accuracy
svm_accuracy = accuracy_score(y_test,svm_predict)
print(f"Using SVM with RBF kernel we get an accuracy of {round(svm_accuracy*100,2)}%")

cm=confusion_matrix(y_test,svm_predict)
conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])
print("confusion matrix:\n", cm)
print(classification_report(y_test,svm_predict))

"""# **Random Forest Classification (RFC) Model**"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=25 ,criterion='entropy', random_state=42)
#train the model
classifier.fit(X_train,y_train)
#prediction
y_pred=classifier.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
from sklearn.metrics import accuracy_score 
rfc_accuracy = accuracy_score(y_test, y_pred)
print ("Accuracy : ", accuracy_score(y_test, y_pred))
print("confusion matrix:\n", cm)

"""# **Logistic Regression Method**"""

######Logistic Regression Method

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, StratifiedKFold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import recall_score,precision_score,classification_report,roc_auc_score,roc_curve

lr = LogisticRegression()
model = lr.fit(X_train, y_train)
lr_predict = lr.predict(X_test)
lr_conf_matrix = confusion_matrix(y_test, lr_predict)
lr_accuracy = accuracy_score(y_test, lr_predict)
print("confussion matrix")
print(lr_conf_matrix)
print("\n")
print("Accuracy of Logistic Regression:",lr_accuracy*100,'\n')
print(classification_report(y_test,lr_predict))

"""# **Cross Validation**"""

##K folds
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
# create dataset
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1)
# prepare the cross-validation procedure
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)

##CV score for logistic regression
model = LogisticRegression()
# evaluate model
scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
cv_score_lr = scores.mean()
print('cv_score: %.3f (%.3f)' % (mean(scores), std(scores)))

##CV score for Random Forest
model = RandomForestClassifier()
# evaluate model
scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
# report performance
cv_score_rfc = scores.mean()
print('cv_score: %.3f (%.3f)' % (mean(scores), std(scores)))

##CV score for SVM
clf = svm.SVC(kernel='linear', C=1, random_state=42)
scores = cross_val_score(clf, X, y, cv=cv)
cv_score_svm = scores.mean()
print('cv_score: %.3f (%.3f)' % (mean(scores), std(scores)))

##CV score for KNN
knn_classifier = KNeighborsClassifier(n_neighbors = 18)
score=cross_val_score(knn_classifier,X,y,cv=cv)
cv_score_knn = score.mean()
print('cv_score: %.3f (%.3f)' % (mean(scores), std(scores)))

##plotting cv scores of respective models
cv_result = []
cv_result.append(cv_score_svm)
cv_result.append(cv_score_rfc)
cv_result.append(cv_score_lr)
cv_result.append(cv_score_knn)
cv_results = pd.DataFrame(cv_result)
print(cv_result)
cv_results = pd.DataFrame({"Cross Validation Means":cv_result, "ML Models":[ "SVC", "RandomForestClassifier", "LogisticRegression", "KNeigborsClassifier"]})

g = sns.barplot(x="Cross Validation Means", y = "ML Models", data=cv_results)
g.set_xlabel("Means Accuracy")
g.set_title("Cross Validation Scores")
plt.show()

comparison = pd.DataFrame({
    "Logistic regression ":{'Accuracy': lr_accuracy},
    "K-nearest neighbours ":{'Accuracy':knn_accuracy},
    "Random Forest ":{'Accuracy':rfc_accuracy},
    "Support vector machine ":{'Accuracy':svm_accuracy}
}).T
fig = plt.gcf()
fig.set_size_inches(15, 15)
titles = ['Accuracy']
for title,label in enumerate(comparison.columns):
    plt.subplot(2,2,title+1)
    sns.barplot(x=comparison.index, y = comparison[label], data=comparison)
    plt.xticks(fontsize=10)
    plt.title(titles[title])
plt.show()